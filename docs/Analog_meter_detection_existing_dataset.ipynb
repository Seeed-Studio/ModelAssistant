{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Analog-meter-detection-existing-dataset.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOhQCIODUHCCcfLgK8CTdFt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Seeed-Studio/Edgelab/blob/master/docs/Analog_meter_detection_existing_dataset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train and deploy an analog meter reading detection model with existing dataset"
      ],
      "metadata": {
        "id": "s3pa013PcXoh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This notebook will guide you to train an analog meter reading detection model with an existing dataset!"
      ],
      "metadata": {
        "id": "4wCnaloE6aew"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Configure host environment"
      ],
      "metadata": {
        "id": "tiVpbLJRMvZS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 1.** Choose **GPU** in **Runtime** if not already selected by navigating to `Runtime --> Change Runtime Type --> Hardware accelerator --> GPU`"
      ],
      "metadata": {
        "id": "6VrcRq-U6G2v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 2.** Clone Edgelab repo and access it"
      ],
      "metadata": {
        "id": "K8prsdf8u8Mv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd ~\n",
        "!git clone https://github.com/Seeed-Studio/Edgelab\n",
        "%cd Edgelab"
      ],
      "metadata": {
        "id": "ZfnLgOjgu8ng"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 3.** Configure the environment by running the following script which will download and install the relevant dependencies"
      ],
      "metadata": {
        "id": "xPagIdHSaY6n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python tools/env_config.py"
      ],
      "metadata": {
        "id": "C32Eow5zafe_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 4.** After the relevant environment configuration is completed, the path of this project needs to be configured in the PYTHONPATH environment variable. Configure it as follows"
      ],
      "metadata": {
        "id": "FN2ixxSEE2K_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "home = os.environ['HOME']\n",
        "os.environ['PYTHONPATH'] = f'{home}/Edgelab'"
      ],
      "metadata": {
        "id": "hiyU2Po6LeWp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Generate firmware image\n",
        "\n",
        "We need to generate a firmware image for the Grove - Vision AI to support the analog meter reading detection model because the default firmware which is pre-installed out-of-the-box does not support it."
      ],
      "metadata": {
        "id": "_FmAXRZBDm1m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you want to download an already pre-compiled firmware, please follow the links below. But we recommend you to compile the firmware from source, so that you will have the latest firmware always.\n",
        "\n",
        "- [Grove - Vision AI Module](https://files.seeedstudio.com/wiki/Edgelab/uf2/grove-vision-ai-firmware.uf2)\n",
        "- [SenseCAP A1101](https://files.seeedstudio.com/wiki/Edgelab/uf2/sensecap-A1101-firmware.uf2)"
      ],
      "metadata": {
        "id": "LY_3XyBXFdZc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 1.** Download GNU Development Toolkit"
      ],
      "metadata": {
        "id": "SQoLqgXxFkeO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd ~\n",
        "!wget https://github.com/foss-for-synopsys-dwc-arc-processors/toolchain/releases/download/arc-2020.09-release/arc_gnu_2020.09_prebuilt_elf32_le_linux_install.tar.gz"
      ],
      "metadata": {
        "id": "0ei9sSIfFpWE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 2.** Extract the file"
      ],
      "metadata": {
        "id": "U9neMfXoFvhD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!tar -xvf arc_gnu_2020.09_prebuilt_elf32_le_linux_install.tar.gz"
      ],
      "metadata": {
        "id": "7iA1fOR7F30y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 3:** Add **arc_gnu_2020.09_prebuilt_elf32_le_linux_install/bin** to **PATH**"
      ],
      "metadata": {
        "id": "yty9F8gGF-wj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ['PATH'] += f':{home}/arc_gnu_2020.09_prebuilt_elf32_le_linux_install/bin'"
      ],
      "metadata": {
        "id": "eXx8WlTVGCow"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 4:** Execute the following to install required dependencies "
      ],
      "metadata": {
        "id": "sjxC3p8kFMw5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd\n",
        "!wget -4c https://ftp.gnu.org/gnu/glibc/glibc-2.29.tar.gz\n",
        "!tar -zxvf glibc-2.29.tar.gz\n",
        "!apt-get install gawk bison -y\n",
        "%cd glibc-2.29 \n",
        "%mkdir build\n",
        "%cd build\n",
        "!../configure --prefix=/usr/local --disable-sanity-checks\n",
        "!make -j8\n",
        "!make install\n",
        "%cd /lib/x86_64-linux-gnu\n",
        "!cp /usr/local/lib/libm-2.29.so /lib/x86_64-linux-gnu/\n",
        "!ln -sf libm-2.29.so libm.so.6"
      ],
      "metadata": {
        "id": "QXoNc1Q4JD_J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 5:** Navigate to the following repo of Edgelab"
      ],
      "metadata": {
        "id": "_fAvHImHSdiZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd ~/Edgelab/examples/vision_ai"
      ],
      "metadata": {
        "id": "TgiH10YjLppz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 6:** Download related tools\n"
      ],
      "metadata": {
        "id": "PMlFGZSbTR3c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!make download"
      ],
      "metadata": {
        "id": "MP4F6ULjTS-I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 7:** Compile the firmware according to your hardware "
      ],
      "metadata": {
        "id": "5TJ7sTcTTYm2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- For Grove - Vision AI Module"
      ],
      "metadata": {
        "id": "GckkHv9GTb5k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!make HW=grove_vision_ai APP=meter\n",
        "!make flash"
      ],
      "metadata": {
        "id": "kKeUCBVHTnbw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- For SenseCAP A1101"
      ],
      "metadata": {
        "id": "psGhUcVIY2qy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!make HW=sensecap_vision_ai APP=meter\n",
        "!make flash"
      ],
      "metadata": {
        "id": "Yug49NEGY6oa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The above will generate **output.img** inside **~/Edgelab/examples/vision_ai/tools/image_gen_cstm/output/** directory"
      ],
      "metadata": {
        "id": "1paJ3XZXZnZ-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 8:** Generate firmware image **firmware.uf2** file so that we can later flash it directly to the Grove - Vision AI Module/ SenseCAP A1101"
      ],
      "metadata": {
        "id": "eJWaZ_KtZ54S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 tools/ufconv/uf2conv.py -t 0 -c tools/image_gen_cstm/output/output.img -o firmware.uf2"
      ],
      "metadata": {
        "id": "IZ6nY4pQaG_K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This will generate **firmware.uf2** inside **~/Edgelab/examples/vision_ai** directory\n",
        "\n"
      ],
      "metadata": {
        "id": "QFLhPf3Hac7P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 9:** Download the **firmware.uf2** file"
      ],
      "metadata": {
        "id": "Wd2znoeFbJON"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download(\"firmware.uf2\")"
      ],
      "metadata": {
        "id": "4F0rVRXhahV0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Prepare dataset"
      ],
      "metadata": {
        "id": "iebJSPm-NHfh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have already prepared a ready-to-use dataset for your convenience."
      ],
      "metadata": {
        "id": "FUs90xSecXSB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 1.** Create a new folder named **datasets** inside the home directory"
      ],
      "metadata": {
        "id": "A4NeT1f7Czo4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%mkdir -p ~/datasets\n",
        "%cd ~"
      ],
      "metadata": {
        "id": "gjQUG_5lAz_A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 2.**  Download an already prepared dataset"
      ],
      "metadata": {
        "id": "f_WiDo23clGZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://files.seeedstudio.com/wiki/Edgelab/meter.zip"
      ],
      "metadata": {
        "id": "_7KHU1zlDJrv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 3.**  Unzip and move to datasets folder that we created before"
      ],
      "metadata": {
        "id": "EEF1UX2ocxMn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip -q -n ~/meter.zip -d ~/datasets"
      ],
      "metadata": {
        "id": "95PhQ3t_c_Yp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Start training"
      ],
      "metadata": {
        "id": "CeUCh_ocdV5y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Configuration file \n",
        "\n",
        "We will choose the profile according to the task that we want to implement. We have prepared preconfigured files inside the the [configs](https://github.com/Seeed-Studio/edgelab/tree/master/configs) folder.\n",
        "\n",
        "For our meter reading detection example, we will use [pfld_mv2n_112.py](https://github.com/Seeed-Studio/Edgelab/blob/master/configs/pfld/pfld_mv2n_112.py) config file. This file will be mainly used to configure the dataset for training including the dataset location."
      ],
      "metadata": {
        "id": "sZ030qVodtUN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we use the previously downloaded dataset for training. Execute the following command inside the activated conda virtual environment terminal to start training an end-to-end analog meter reading detection model."
      ],
      "metadata": {
        "id": "Eq1qGpKleE0N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd ~/Edgelab\n",
        "!~/anaconda3/envs/edgelab/bin/python tools/train.py mmpose configs/pfld/pfld_mv2n_112.py --gpus=1 --cfg-options total_epochs=5"
      ],
      "metadata": {
        "id": "jDIZh8Y_dXqh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "After the training is completed, a model weight file will be generated under **~/edgelab/work_dirs/pfld_mv2n_112/exp1/latest.pth**. Remember the path to this file, which will be used when exporting the model."
      ],
      "metadata": {
        "id": "0i_NGNEgs8z-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The format of the above command looks like below\n",
        "\n",
        "```sh\n",
        "~/anaconda3/envs/edgelab/bin/python tools/train.py <task_type> <config_file_location> --gpus=<cpu_or_gpu> --cfg-options total_epochs=<number_of_epochs>\n",
        "```\n",
        "\n",
        "where:\n",
        "\n",
        "- [task_type][link text](https://) refers to either **mmcls** for classfication, **mmdet** for detection and **mmpose** for pose estimation\n",
        "- [config_file_location] refers to the path where the model configuration is located \n",
        "- [cpu_or_gpu] refers to specifying whether you want to train on CPU or GPU. Type **0** CPU and **1** for GPU\n",
        "- --cfg-options total_epochs=[number_of_epochs] refers to the number of training cycles"
      ],
      "metadata": {
        "id": "9BFh45xWeMu9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Export PyTorch to TFLite"
      ],
      "metadata": {
        "id": "XDTRTOjqe74q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "After the model training is completed, you can export the **.pth** file to the **TFLite** file format. This is important because TFLite format is more optimized to run on low power hardware. Assuming that the environment is in this project path, you can export the models you have trained before to the TFLite format by running the following command:\n",
        "\n"
      ],
      "metadata": {
        "id": "lpPtbM8Be_N6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python tools/export.py configs/pfld/pfld_mv2n_112.py --weights work_dirs/pfld_mv2n_112/exp1/latest.pth --data ~/datasets/meter/train/images"
      ],
      "metadata": {
        "id": "5HCCaDQAfFV2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This will generate a **latest_int8.tflite** file inside **~/Edgelab/work_dirs/pfld_mv2n_112/exp1** directory"
      ],
      "metadata": {
        "id": "aAneExFVfH_3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The format of the above command looks like below\n",
        "\n",
        "```sh\n",
        "python tools/export.py configs/xxx/xxx.py --weights <location_to_pth_from_training> --data <location_to_images_directory_of__train_or_val>\n",
        "```\n",
        "\n",
        "where:\n",
        "\n",
        "- configs/xxx/xxx.py refers to the location of the configuration file correcsponsing to the AI model\n",
        "- --weights refers to the the .pth file that was generated during training\n",
        "- --data refers to the images directory of either train or val"
      ],
      "metadata": {
        "id": "CFYKOP9KfUkb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Convert TFLite to UF2"
      ],
      "metadata": {
        "id": "Ehr7I4MOfiWF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we will convert the generated TFLite file to a UF2 file so that we can directly flash the UF2 file into Grove - Vision AI Module and SenseCAP A1101\n",
        "\n"
      ],
      "metadata": {
        "id": "pCFlwF-rfk6P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 1:** Execute the following"
      ],
      "metadata": {
        "id": "viwshv1BzZ93"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python ~/Edgelab/examples/vision_ai/tools/ufconv/uf2conv.py -f GROVEAI -t 1 -c ~/Edgelab/work_dirs/pfld_mv2n_112/exp1/latest_int8.tflite -o model.uf2"
      ],
      "metadata": {
        "id": "5P_5vU-zfjwJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This will generate a **model.uf2** file inside **~/Edgelab/examples/vision_ai** directory\n",
        "\n"
      ],
      "metadata": {
        "id": "tSqePhFJfpCK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here you only change the location of the TFLite model such as **~/Edgelab/work_dirs/pfld_mv2n_112/exp1/latest_int8.tflite**\n",
        "\n"
      ],
      "metadata": {
        "id": "OPEcHbptftc3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 2:** Download the uf2 model file"
      ],
      "metadata": {
        "id": "A3CywUzqzi9L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "files.download(\"model.uf2\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "QvO62mjtyRKZ",
        "outputId": "c0a84238-7991-4d30-99b9-36ea27da98cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_3dd099b3-834d-4fcb-89df-4446849e60db\", \"model.uf2\", 481280)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. Deploy and inference"
      ],
      "metadata": {
        "id": "c0ZkP3MEfyJ5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Flash firmware and model\n",
        "\n",
        "This explains how you can flash the previously generated firmware (firmware.uf2) and the model file (model.uf2) to Grove - Vision AI Module and SenseCAP A1101.\n",
        "\n",
        "**Step 1.** Connect Grove - Vision AI Module/ SenseCAP A1101 to PC by using USB Type-C cable \n",
        "\n",
        "<div align=center><img width=1000 src=\"https://files.seeedstudio.com/wiki/SenseCAP-A1101/45.png\"/></div>\n",
        "\n",
        "**Step 2.** Double click the boot button to enter **boot mode**\n",
        "\n",
        "<div align=center><img width=1000 src=\"https://files.seeedstudio.com/wiki/SenseCAP-A1101/46.png\"/></div>\n",
        "\n",
        "**Step 3:** After this you will see a new storage drive shown on your file explorer as **GROVEAI** for **Grove - Vision AI Module** and as **VISIONAI** for **SenseCAP A1101**\n",
        "\n",
        "<div align=center><img width=500 src=\"https://files.seeedstudio.com/wiki/SenseCAP-A1101/62.jpg\"/></div>\n",
        "\n",
        "**Step 4:** Drag and drop the previous **firmware.uf2** at first, and then the **model.uf2** file to **GROVEAI** or **VISIONAI** \n",
        "\n",
        "Once the copying is finished **GROVEAI** or **VISIONAI** drive will disapper. This is how we can check whether the copying is successful or not."
      ],
      "metadata": {
        "id": "bt_TqW_Bf0Ip"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8. View live detection results"
      ],
      "metadata": {
        "id": "IgwvGiWu0ALz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 1:** After loading the firmware and connecting to PC, visit [this URL](https://files.seeedstudio.com/grove_ai_vision/index.html)\n",
        "\n",
        "**Step 2:** Click **Connect** button. Then you will see a pop up on the browser. Select **Grove AI - Paired** and click **Connect**\n",
        "  \n",
        "<div align=center><img width=800 src=\"https://files.seeedstudio.com/wiki/Edgelab/meter-own-github/13.jpg\"/></div>\n",
        "\n",
        "<div align=center><img width=400 src=\"https://files.seeedstudio.com/wiki/Edgelab/meter-own-github/12.png\"/></div>\n",
        "\n",
        "Upon successful connection, you will see a live preview from the camera. Here the camera is pointed at an analog meter.\n",
        "\n",
        "<div align=center><img width=800 src=\"https://files.seeedstudio.com/wiki/Edgelab/meter-own-github/14.png\"/></div>\n",
        "\n",
        "Now we need to set 3 points which is the center point, start point and the end point. \n",
        "\n",
        "**Step 3:** Click on **Set Center Point** and click on the center of the meter. you will see a pop up confirm the location and press **OK**\n",
        "\n",
        "<div align=center><img width=800 src=\"https://files.seeedstudio.com/wiki/Edgelab/meter-own-github/15.png\"/></div>\n",
        "\n",
        "You will see the center point is already recorded\n",
        "\n",
        "<div align=center><img width=800 src=\"https://files.seeedstudio.com/wiki/Edgelab/meter-own-github/16.png\"/></div>\n",
        "\n",
        "**Step 4:** Click on **Set Start Point** and click on the first indicator point. you will see a pop up confirm the location and press **OK**\n",
        "\n",
        "<div align=center><img width=800 src=\"https://files.seeedstudio.com/wiki/Edgelab/meter-own-github/17.png\"/></div>\n",
        "\n",
        "You will see the first indicator point is already recorded\n",
        "\n",
        "<div align=center><img width=800 src=\"https://files.seeedstudio.com/wiki/Edgelab/meter-own-github/18.png\"/></div>\n",
        "\n",
        "**Step 5:** Click on **Set End Point** and click on the last indicator point. you will see a pop up confirm the location and press **OK**\n",
        "\n",
        "<div align=center><img width=800 src=\"https://files.seeedstudio.com/wiki/Edgelab/meter-own-github/19.png\"/></div>\n",
        "\n",
        "You will see the last indicator point is already recorded\n",
        "\n",
        "<div align=center><img width=800 src=\"https://files.seeedstudio.com/wiki/Edgelab/meter-own-github/20.png\"/></div>\n",
        "\n",
        "**Step 6:** Set the measuring range according to the first digit and last digit of the meter. For example, he we set as **From:0 To 0.16**\n",
        "\n",
        "<div align=center><img width=800 src=\"https://files.seeedstudio.com/wiki/Edgelab/meter-own-github/21.png\"/></div>\n",
        "\n",
        "**Step 7:** Set the number of decimal places that you want the result to display. Here we set as 2\n",
        "\n",
        "<div align=center><img width=800 src=\"https://files.seeedstudio.com/wiki/Edgelab/meter-own-github/22.png\"/></div>\n",
        "\n",
        "Finally you can see the live meter reading results as follows\n",
        "\n",
        "<div align=center><img width=800 src=\"https://files.seeedstudio.com/wiki/Edgelab/meter-own-github/meter.gif\"/></div>"
      ],
      "metadata": {
        "id": "R1JH8ans0GWw"
      }
    }
  ]
}